# -*- coding: utf-8 -*-
"""
.. _tuner:

Tuner
=================

AgentScope provides a ``tuner`` module to train agent applications using reinforcement learning (RL).
This tutorial will guide you through the process of using the ``tuner`` module to enhance an agent's performance on a specific task, which includes:

- Introducing the main components of the ``tuner`` module.
- Demonstrating how to implement the necessary code components for tuning.
- Showing how to setup and run the tuning process.

Main Components
~~~~~~~~~~~~~~~~~~~
The ``tuner`` module introduce three main components necessary for training an agent application using RL:

- **Task Dataset**: A collection of tasks used for training and evaluating the agent application.
- **Workflow Function**: A function that internally contains the agent application being tuned.
- **Judge Function**: A function that evaluates the agent's performance on a given task and provides a reward signal for tuning.

Besides these components, the ``tuner`` module also provides some configuration classes to customize the tuning process, including:

- **TunerChatModel**: A configurable chat model for only for tuning purposes, fully compatible with AgentScope's ``OpenAIChatModel``.
- **Algorithm**: The RL algorithm used for tuning, e.g., GRPO, PPO, etc.


How to Implement
~~~~~~~~~~~~~~~~~~~
Here we will implement a simple math agent that can be trained using the ``tuner`` module.


Task Dataset
--------------------
The task dataset contains a collection of tasks for training and evaluating your agent application.

The dataset should be organized in huggingface `datasets <https://huggingface.co/docs/datasets/quickstart>`_ format and can be loaded using the ``datasets.load_dataset`` function. For example:

.. code-block:: text

    my_dataset/
        ├── train.jsonl  # samples for training
        └── test.jsonl   # samples for evaluation

Suppose your `train.jsonl` contains samples like:

.. code-block:: json

    {"question": "What is 2 + 2?", "answer": "4"}
    {"question": "What is 4 + 4?", "answer": "8"}


Workflow Function
--------------------
The workflow function that defines how the agents interacts with the environment and makes decisions. All workflow functions should follow the input/output signature defined in ``agentscope.tuner.WorkflowType``.

Below is an example of a simple workflow function that uses a ReAct agent to answer math questions.
"""

from typing import Dict, Optional
from agentscope.agent import ReActAgent
from agentscope.formatter import OpenAIChatFormatter
from agentscope.message import Msg
from agentscope.model import ChatModelBase
from agentscope.tuner import WorkflowOutput


async def example_workflow_function(
    task: Dict,
    model: ChatModelBase,
    auxiliary_models: Optional[Dict[str, ChatModelBase]] = None,
) -> WorkflowOutput:
    """An example workflow function for tuning.

    Args:
        task (dict): The task information.
        model (ChatModelBase): The chat model used by the agent.
        auxiliary_models (Optional[Dict[str, ChatModelBase]]): Additional
            chat models, generally used to simulate the behavior of other
            non-training agents in multi-agent scenarios.

    Returns:
        WorkflowOutput: The output generated by the workflow.
    """
    agent = ReActAgent(
        name="react_agent",
        sys_prompt="You are a helpful math problem solving agent.",
        model=model,
        formatter=OpenAIChatFormatter(),
    )

    response = await agent.reply(
        msg=Msg(
            "user",
            task["question"],
            role="user",
        ),  # extract question from task
    )

    return WorkflowOutput(  # put the response into WorkflowOutput
        response=response,
    )


# %%
# You can run this workflow function directly with a task dictionary and a chat model.
# For example:

import asyncio
import os
from agentscope.model import DashScopeChatModel

task = {"question": "What is 123 plus 456?", "answer": "579"}
model = DashScopeChatModel(
    model_name="qwen-max",
    api_key=os.environ["DASHSCOPE_API_KEY"],
)
workflow_output = asyncio.run(example_workflow_function(task, model))
assert isinstance(
    workflow_output.response,
    Msg,
), "In this example, the response should be a Msg instance."
print("\nWorkflow response:", workflow_output.response.get_text_content())

# %%
#
# Judge Function
# --------------------
# The judge function evaluates the agent's performance on a given task and provides a reward signal for tuning.
# All judge functions should follow the input/output signature defined in ``agentscope.tuner.JudgeType``.
# Below is an example of a simple judge function that compares the agent's response with the ground truth answer.

from typing import Any
from agentscope.tuner import JudgeOutput


async def example_judge_function(
    task: Dict,
    response: Any,
    auxiliary_models: Optional[Dict[str, ChatModelBase]] = None,
) -> JudgeOutput:
    """A very simple judge function only for demonstration.

    Args:
        task (Dict): The task information.
        response (Any): The response field from the WorkflowOutput.
        auxiliary_models (Optional[Dict[str, ChatModelBase]]): Additional
            chat models for LLM-as-a-Judge purpose.
    """
    ground_truth = task["answer"]
    reward = 1.0 if ground_truth in response.get_text_content() else 0.0
    return JudgeOutput(reward=reward)


judge_output = asyncio.run(
    example_judge_function(
        task,
        workflow_output.response,
    ),
)
print(f"Judge reward: {judge_output.reward}")

# %%
# .. tip:: You can leverage existing `MetricBase <https://github.com/agentscope-ai/agentscope/blob/main/src/agentscope/evaluate/_metric_base.py>`_ implementations in your judge function to compute more sophisticated metrics and combine them into a composite reward.
#
# How to Run
# ~~~~~~~~~~~~~~~
# Finally, you can set up and run the tuning process using the ``tuner`` module.
# Before starting the tuning, make sure you have `Trinity-RFT <https://github.com/modelscope/Trinity-RFT>`_ installed in your environment, as it is a dependency for the tuning process.
#
# Below is an example of how to configure and start the tuning process.
#
# .. code-block:: python
#
#        from agentscope.tuner import tune, Algorithm, Dataset, TunerChatModel
#        # your workflow / judge function here...
#
#        if __name__ == "__main__":
#            dataset = Dataset(path="my_dataset", split="train")
#            model = TunerChatModel(model_path="Qwen/Qwen3-0.6B", max_model_len=16384)
#            algorithm = Algorithm(
#                algorithm_type="multi_step_grpo",
#                group_size=8,
#                batch_size=32,
#                learning_rate=1e-6,
#            )
#            tune(
#                workflow_func=example_workflow_function,
#                judge_func=example_judge_function,
#                model=model,
#                train_dataset=dataset,
#                algorithm=algorithm,
#            )
#
# Here, we use ``Dataset`` to load the training dataset, ``TunerChatModel`` to
# initialize the trainable model, and ``Algorithm`` to specify the RL algorithm
# and its hyperparameters.
#
# .. note:: This example is for demonstration purposes only. Please refer to the code in `Tune ReActAgent <https://github.com/agentscope-ai/agentscope/tree/main/examples/tuner/react_agent>`_ for a complete and runnable example.
#
# .. tip::
#   The ``tune`` function is based on `Trinity-RFT <https://github.com/modelscope/Trinity-RFT>`_ and it converts the input parameters into a YAML configuration internally.
#   Advanced users can ignore ``model``, ``train_dataset``, ``algorithm`` arguments and provide a configuration file path pointing to a YAML file using the ``config_path`` argument instead.
#   We recommend using the configuration file approach for fine-grained control over the training process and leveraging advanced features provided by Trinity-RFT.
#   You can refer to the Trinity-RFT `Configuration Guide <https://modelscope.github.io/Trinity-RFT/en/main/tutorial/trinity_configs.html>`_ for more details on configuration options.
#
# You can save the above code in a file named ``main.py`` and run it with the following commands:
#
# .. code-block:: bash
#
#        ray start --head
#        python main.py
#
# The checkpoint and logs will automatically be saved to the ``checkpoints/AgentScope`` directory under the current working directory and each run will be save in a sub-directory suffixed with current timestamp.
# You can found the tensorboard logs inside ``monitor/tensorboard`` of the checkpoint directory.
#
# .. code-block:: text
#
#        your_workspace/
#            └── checkpoints/
#                └──AgentScope/
#                    └── Experiment-20260104185355/  # each run saved in a sub-directory with timestamp
#                        ├── monitor/
#                        │   └── tensorboard/  # tensorboard logs
#                        └── global_step_x/    # saved model checkpoints at step x
#
