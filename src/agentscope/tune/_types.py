# -*- coding: utf-8 -*-
"""Types used in agentscope tuner.

Workflow: An agent workflow function type for tuning.

Args:
    task (Dict):
        The task information for the workflow run.
    model (ChatModelBase):
        The primary chat model used in the workflow, this is the main model
        being tuned.
    auxiliary_models (Dict[str, ChatModelBase]):
        A dictionary of additional chat models available for LLM-as-a-Judge
        usage. The keys are model names, and the values are the corresponding
        ChatModelBase instances. Note that these auxiliary models are not tuned
        during the workflow.
Returns:
    WorkflowOutput:
        The reward obtained from the workflow function.


Judge: A judge function type for tuning.

Args:
    task (Dict):
        The task information for the corresponding workflow.
    workflow_output (WorkflowOutput):
        The output generated by the corresponding workflow.
    auxiliary_models (Dict[str, ChatModelBase]):
        A dictionary of additional chat models available for LLM-as-a-Judge
        usage. The keys are model names, and the values are the corresponding
        ChatModelBase instances.
Returns:
    JudgeOutput:
        The reward value assigned by the judge function.
"""

from typing import (
    Dict,
    Callable,
    Awaitable,
    Any,
    Optional,
)
from pydantic import BaseModel, Field

from ..model import ChatModelBase


class WorkflowOutput(BaseModel):
    """The output of a workflow function."""

    reward: Optional[float] = Field(
        description=(
            "The reward obtained from the workflow function. "
            "Used for direct reward output."
        ),
        default=None,
    )
    response: Optional[Any] = Field(
        description=(
            "The response generated by the workflow function. "
            "Used as judge input."
        ),
        default=None,
    )

    metrics: Optional[Dict[str, float]] = Field(
        description="Metrics from the workflow function.",
        default=None,
    )


WorkflowType = Callable[
    [Dict, ChatModelBase, Dict[str, ChatModelBase]],
    Awaitable[WorkflowOutput],
]


class JudgeOutput(BaseModel):
    """The output of a judge function."""

    reward: float = Field(
        description="The reward value assigned by the judge function.",
    )

    metrics: Optional[Dict[str, float]] = Field(
        description="Metrics from the judge function.",
        default=None,
    )


JudgeType = Callable[
    [Dict, Any, Dict[str, ChatModelBase]],
    Awaitable[JudgeOutput],
]


class Dataset(BaseModel):
    """Dataset information for tuning.
    Compatible with huggingface dataset format.
    Agentscope will load the dataset from the given path using
    `datasets.load_dataset`.
    """

    path: str = Field(
        description="Path to your dataset.",
    )
    name: Optional[str] = Field(
        description="The name of the dataset configuration.",
        default=None,
    )
    split: Optional[str] = Field(
        description="The dataset split to use.",
        default="train",
    )

    def preview(self, n: int = 5) -> Dict:
        """Preview the dataset information.

        Args:
            n (int): Number of samples to preview.
        """
        try:
            from datasets import load_dataset
        except ImportError as e:
            raise ImportError(
                "The `datasets` library is not installed. "
                "Please install it with `pip install datasets`.",
            ) from e
        import json

        ds = load_dataset(
            path=self.path,
            name=self.name,
            split=self.split,
        )
        samples = ds[:n]
        print(json.dumps(samples, indent=2))
        return samples
