# -*- coding: utf-8 -*-
"""Types used in agentscope tuner.

Workflow: An agent workflow function type for tuning.

Args:
    task (Dict):
        The task information for the workflow run.
    model (ChatModelBase):
        The primary chat model used in the workflow, this is the main model
        being tuned.
    auxiliary_models (Dict[str, ChatModelBase]):
        A dictionary of additional chat models available for LLM-as-a-Judge
        usage. The keys are model names, and the values are the corresponding
        ChatModelBase instances. Note that these auxiliary models are not tuned
        during the workflow.
Returns:
    WorkflowOutput:
        The reward obtained from the workflow function.


Judge: A judge function type for tuning.

Args:
    task (Dict):
        The task information for the corresponding workflow.
    workflow_output (WorkflowOutput):
        The output generated by the corresponding workflow.
    auxiliary_models (Dict[str, ChatModelBase]):
        A dictionary of additional chat models available for LLM-as-a-Judge
        usage. The keys are model names, and the values are the corresponding
        ChatModelBase instances.
Returns:
    JudgeOutput:
        The reward value assigned by the judge function.
"""

from typing import (
    Dict,
    Callable,
    Awaitable,
    Any,
    Optional,
    TYPE_CHECKING,
)
from pydantic import BaseModel, Field

from ..model import ChatModelBase, OpenAIChatModel

if TYPE_CHECKING:
    import openai


class WorkflowOutput(BaseModel):
    """The output of a workflow function."""

    reward: Optional[float] = Field(
        description=(
            "The reward obtained from the workflow function. "
            "Used for direct reward output."
        ),
        default=None,
    )
    response: Optional[Any] = Field(
        description=(
            "The response generated by the workflow function. "
            "Used as judge input."
        ),
        default=None,
    )

    metrics: Optional[Dict[str, float]] = Field(
        description="Metrics from the workflow function.",
        default=None,
    )


WorkflowType = Callable[
    [Dict, ChatModelBase, Dict[str, ChatModelBase]],
    Awaitable[WorkflowOutput],
]


class JudgeOutput(BaseModel):
    """The output of a judge function."""

    reward: float = Field(
        description="The reward value assigned by the judge function.",
    )

    metrics: Optional[Dict[str, float]] = Field(
        description="Metrics from the judge function.",
        default=None,
    )


JudgeType = Callable[
    [Dict, Any, Dict[str, ChatModelBase]],
    Awaitable[JudgeOutput],
]


class Dataset(BaseModel):
    """Dataset information for tuning.
    Compatible with huggingface dataset format.
    Agentscope will load the dataset from the given path using
    `datasets.load_dataset`.
    """

    path: str = Field(
        description="Path to your dataset.",
    )
    name: Optional[str] = Field(
        description="The name of the dataset configuration.",
        default=None,
    )
    split: Optional[str] = Field(
        description="The dataset split to use.",
        default="train",
    )

    def preview(self, n: int = 5) -> Dict:
        """Preview the dataset information.

        Args:
            n (int): Number of samples to preview.
        """
        try:
            from datasets import load_dataset
        except ImportError as e:
            raise ImportError(
                "The `datasets` library is not installed. "
                "Please install it with `pip install datasets`.",
            ) from e
        import json

        ds = load_dataset(
            path=self.path,
            name=self.name,
            split=self.split,
        )
        samples = ds[:n]
        print(json.dumps(samples, indent=2))
        return samples


class TunerChatModel(OpenAIChatModel):
    """Chat model for tuning.
    This model is specifically designed for reinforcement learning,
    and the `client` attribute is expected to be set during the tuning
    process. Please don't use this chat model for other purposes.
    """

    def __init__(
        self,
        model_path: str,
        max_model_len: int,
        temperature: float = 1.0,
        top_p: float = 1.0,
        max_tokens: int = 8192,
        enable_thinking: Optional[bool] = None,
        tensor_parallel_size: int = 1,
        engine_num: int = 1,
        tool_call_parser: str = "hermes",
        reasoning_parser: str = "deepseek_r1",
    ):
        """Initialize the tuner chat model.

        Args:
            model_path (str): The path to the model checkpoint.
            max_model_len (int): The maximum length of the model, including
                context and generated tokens.
            temperature (float): Sampling temperature.
            top_p (float): Nucleus sampling probability.
            max_tokens (int): Maximum tokens for generation.
            enable_thinking (Optional[bool]): Whether to enable thinking
                capability. Only applicable for Qwen3 series models.
            tensor_parallel_size (int): The tensor parallel size for
                model inference.
            engine_num (int): The number of engines for model inference.
        """
        super().__init__(
            model_name=model_path,
            api_key="EMPTY",
            stream=False,  # RL training does not support streaming
        )

        self.generate_kwargs["temperature"] = temperature
        self.generate_kwargs["top_p"] = top_p
        self.generate_kwargs["max_tokens"] = max_tokens
        if enable_thinking is not None:
            if "chat_template_kwargs" not in self.generate_kwargs:
                self.generate_kwargs["chat_template_kwargs"] = {}
            assert isinstance(
                self.generate_kwargs["chat_template_kwargs"],
                dict,
            ), "chat_template_kwargs must be a dictionary."
            self.generate_kwargs["chat_template_kwargs"][
                "enable_thinking"
            ] = enable_thinking

        self.model_path = model_path
        self.max_model_len = max_model_len
        self.temperature = temperature
        self.top_p = top_p
        self.max_tokens = max_tokens
        self.tensor_parallel_size = tensor_parallel_size
        self.engine_num = engine_num
        self.tool_call_parser = tool_call_parser
        self.reasoning_parser = reasoning_parser
        # set client to None initially, will be set later
        self.client = None  # type: ignore

    def set_openai_client(
        self,
        openai_async_client: "openai.AsyncOpenAI",
    ) -> None:
        """Set the OpenAI async client for the model.

        Args:
            openai_async_client (AsyncOpenAI): The OpenAI async client
            instance
        """
        self.client = openai_async_client

    def get_config(self) -> Dict[str, Any]:
        """Get the model configuration.

        Returns:
            Dict[str, Any]: The model configuration dictionary.
        """
        return {
            "model_path": self.model_path,
            "max_model_len": self.max_model_len,
            "tensor_parallel_size": self.tensor_parallel_size,
            "engine_num": self.engine_num,
            "tool_call_parser": self.tool_call_parser,
            "reasoning_parser": self.reasoning_parser,
            "enable_openai_api": True,
            "enable_auto_tool_choice": True,
        }
